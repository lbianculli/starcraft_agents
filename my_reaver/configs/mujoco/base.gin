# params for generic actor-critic:
# ==========================================
build_mlp.activation = 'tanh'
build_mlp.value_separate = True
build_mlp.obs_shift = True
build_mlp.obs_scale = True
build_mlp.initializer = @tf.initializers.orthogonal()

ACAgent.model_fn = @build_mlp
ACAgent.policy_cls = @MultiPolicy

ACAgent.optimizer = @tf.train.AdamOptimizer()
tf.train.AdamOptimizer.learning_rate = @tf.train.polynomial_decay()
tf.train.polynomial_decay.decay_steps = 20000
tf.train.polynomial_decay.learning_rate = 0.0003
tf.train.polynomial_decay.end_learning_rate = 0.0
tf.train.polynomial_decay.global_step = @tf.train.get_global_step()

ACAgent.value_coef = 0.5
ACAgent.entropy_coef = 0.0

ACAgent.batch_sz = 2
ACAgent.traj_len = 1024

ACAgent.discount = 0.99
ACAgent.gae_lambda = 0.0

ACAgent.clip_rewards = 1.0
ACAgent.clip_grads_norm = 1.0

ACAgent.normalize_advantages = True

# params for A2C:
# ==========================================
# ...

# params for PPO:
# ==========================================
PPOAgent.n_epochs = 10
PPOAgent.minibatch_sz = 64
PPOAgent.clip_ratio = 0.2
PPOAgent.clip_value = 0.0

PPOAgent.gae_lambda = 0.95